{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GhCHf07G3dDu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate datasets peft -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizerFast,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from transformers import EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "NH0ciDGO3lVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bhadresh-savani/distilbert-base-uncased-emotion\"\n",
        "FILE_NAME = '/combined_emotion.csv'\n",
        "OUTPUT_DIR = \"./peft_finetuned_emotion_model\"\n",
        "NUM_LABELS = 6\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "BhaLZd_x3nf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_tokenize_data(file_name, model_name):\n",
        "    print(\"Loading and tokenizing full dataset...\")\n",
        "    df = pd.read_csv(file_name)\n",
        "    print(len(df))\n",
        "    df['emotion'] = df['emotion'].str.lower().str.strip()\n",
        "\n",
        "    label_mapping = {'sad': 'sadness', 'suprise': 'surprise'}\n",
        "    df['emotion'] = df['emotion'].replace(label_mapping)\n",
        "\n",
        "    model_emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
        "    label_to_id = {label: i for i, label in enumerate(model_emotions)}\n",
        "\n",
        "    df['label'] = df['emotion'].map(label_to_id)\n",
        "\n",
        "    df.dropna(subset=['label'], inplace=True)\n",
        "    df = df.reset_index(drop=True)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "\n",
        "    print(\"\\n--- Model Class ID Mappings ---\")\n",
        "    print(\"Label (Class Name) to ID:\")\n",
        "    print(label_to_id)\n",
        "    print(\"\\nID to Label (Class Name):\")\n",
        "    print(id_to_label)\n",
        "\n",
        "    df_full_size = len(df)\n",
        "    print(f\"Cleaned training data size before sampling: {df_full_size}\")\n",
        "    FINAL_SAMPLE_SIZE = 30000\n",
        "\n",
        "    if df_full_size > FINAL_SAMPLE_SIZE:\n",
        "        sample_fraction = FINAL_SAMPLE_SIZE / df_full_size\n",
        "        df = df.groupby('label', group_keys=False).apply(\n",
        "            lambda x: x.sample(int(max(1, len(x) * sample_fraction)), random_state=42)\n",
        "        ).reset_index(drop=True)\n",
        "\n",
        "        if len(df) > FINAL_SAMPLE_SIZE:\n",
        "            df = df.sample(n=FINAL_SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "        print(f\"**SUCCESS: Stratified data reduced to EXACTLY {len(df)} samples for CPU stability and quality.**\")\n",
        "\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "    tokenized_train_dataset = train_test_split['train'].map(tokenize_function, batched=True)\n",
        "    tokenized_eval_dataset = train_test_split['test'].map(tokenize_function, batched=True)\n",
        "\n",
        "    columns_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "    tokenized_train_dataset = tokenized_train_dataset.select_columns(columns_to_keep)\n",
        "    tokenized_eval_dataset = tokenized_eval_dataset.select_columns(columns_to_keep)\n",
        "\n",
        "    return tokenized_train_dataset, tokenized_eval_dataset, tokenizer\n",
        "\n",
        "tokenized_train_dataset, tokenized_eval_dataset, tokenizer = load_and_tokenize_data(FILE_NAME, MODEL_NAME)\n",
        "print(f\"Full training data size: {len(tokenized_train_dataset)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jqOxyPCm3qb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS\n",
        ")\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "h9353S0y3sY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(p):\n",
        "    labels = p.label_ids\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_best_quality2',\n",
        "    num_train_epochs=5,\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.1,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='eval_loss',\n",
        "    fp16=False,\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
        ")\n",
        "\n",
        "print(\"\\nStarting PEFT Fine-Tuning (Training only <1% of parameters)...\")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "t-dnMEJW3uOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIRECTORY = \"./emotions_model_assets\"\n",
        "trainer.model.save_pretrained(SAVE_DIRECTORY)\n",
        "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
        "print(f\"DistilBERT Model and Tokenizer saved to the folder: {SAVE_DIRECTORY}\")"
      ],
      "metadata": {
        "id": "6Er8BrD28_wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_NAME = \"emotions_model_assets\"\n",
        "ZIP_NAME = \"distilbert_emotions_model.zip\"\n",
        "!zip -r $ZIP_NAME $FOLDER_NAME\n",
        "print(f\"Folder successfully zipped as {ZIP_NAME}\")"
      ],
      "metadata": {
        "id": "nre5j2Q09N-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(ZIP_NAME)"
      ],
      "metadata": {
        "id": "FLd3BimY9Qes"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}