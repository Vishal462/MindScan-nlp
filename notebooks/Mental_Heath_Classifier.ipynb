{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uSYpqxuyqZAC"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "from sklearn.utils import class_weight"
      ],
      "metadata": {
        "id": "yclTA2NoqlVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers.models.roberta.modeling_roberta import RobertaForSequenceClassification\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "class WeightedRobertaForSequenceClassification(RobertaForSequenceClassification):\n",
        "    def __init__(self, config, **kwargs):\n",
        "        weights = kwargs.pop('weights', None)\n",
        "        super().__init__(config, **kwargs)\n",
        "        self.loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], dict]:\n",
        "\n",
        "        outputs = super().forward(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        logits = outputs.logits\n",
        "        loss = None\n",
        "\n",
        "        if labels is not None:\n",
        "            if self.config.num_labels == 1:\n",
        "                loss = self.loss_fct(logits.view(-1), labels.float().view(-1))\n",
        "            else:\n",
        "                loss = self.loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits, \"hidden_states\": outputs.hidden_states, \"attentions\": outputs.attentions}"
      ],
      "metadata": {
        "id": "VIUJsUGIqpsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/Mental_health.csv\", engine='python')\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0J06nqsqvWz",
        "outputId": "2316b383-cd19-4a76-8cf1-cd855ebe99f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(53043, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['statement', 'status'])\n",
        "df['statement'] = df['statement'].astype(str).str.strip()\n",
        "df = df[df['statement'].str.len() > 0]\n",
        "df = df.drop_duplicates(subset='statement')\n",
        "\n",
        "X = df['statement'].tolist()\n",
        "y = df['status'].tolist()"
      ],
      "metadata": {
        "id": "eXtpJud3qxaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = sorted(df['status'].unique())\n",
        "label_to_id = {label: i for i, label in enumerate(labels)}\n",
        "id_to_label = {i: label for label, i in label_to_id.items()}\n",
        "y_ids = [label_to_id[label] for label in y]\n",
        "print(\"\\n--- Model Class ID Mappings ---\")\n",
        "print(\"Label (Class Name) to ID:\")\n",
        "print(label_to_id)\n",
        "print(\"\\nID to Label (Class Name):\")\n",
        "print(id_to_label)\n",
        "\n",
        "EVAL_SAMPLE_SIZE = 0.1\n",
        "# Use 100% of the training data\n",
        "X_train, X_eval, y_train, y_eval = train_test_split(\n",
        "    X, y_ids, test_size=EVAL_SAMPLE_SIZE, stratify=y_ids, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "bv_ppAc0AfU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LEN = 128\n",
        "NUM_CLASSES = len(labels)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z_VwK4Q_AlEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = np.unique(y_ids)\n",
        "class_weights_array = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=unique_labels,\n",
        "    y=y_ids\n",
        ")\n",
        "\n",
        "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
        "print(f\"\\nCalculated Class Weights: {class_weights_array}\")\n",
        "\n",
        "\n",
        "print(f\"\\nTotal training samples being used: {len(X_train)}\")\n",
        "print(f\"Evaluation samples (Reduced): {len(X_eval)}\")\n",
        "print(f\"Number of classes: {len(labels)}\")"
      ],
      "metadata": {
        "id": "tQcij6JkA9xW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LEN)"
      ],
      "metadata": {
        "id": "HdGwWcY_q5Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_dict = {'text': X_train, 'label': y_train}\n",
        "eval_data_dict = {'text': X_eval, 'label': y_eval}\n",
        "\n",
        "raw_train_dataset = Dataset.from_dict(train_data_dict)\n",
        "raw_eval_dataset = Dataset.from_dict(eval_data_dict)\n",
        "\n",
        "tokenized_train_dataset = raw_train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
        "tokenized_eval_dataset = raw_eval_dataset.map(tokenize_function, batched=True, remove_columns=['text'] )\n",
        "\n",
        "columns_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "tokenized_train_dataset = tokenized_train_dataset.select_columns(columns_to_keep)\n",
        "tokenized_eval_dataset = tokenized_eval_dataset.select_columns(columns_to_keep)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "iX-BkkPHq7bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'roberta-base'\n",
        "MAX_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "NUM_CLASSES = len(labels)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = WeightedRobertaForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_CLASSES,\n",
        "    weights=class_weights_tensor\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pYPW5bOVAPyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions = np.argmax(p.predictions, axis=1)\n",
        "    macro_f1 = f1_metric.compute(predictions=predictions, references=p.label_ids, average='macro')['f1']\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy_metric.compute(predictions=predictions, references=p.label_ids)['accuracy'],\n",
        "        'macro_f1': macro_f1,\n",
        "    }"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VuuzykrKq9i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results_roberta',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=150,\n",
        "    weight_decay=0.05,\n",
        "    logging_dir='./logs_roberta',\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=250,\n",
        "    load_best_model_at_end=False,\n",
        "    metric_for_best_model='macro_f1',\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "\n",
        "print(\"\\nStarting BERT fine-tuning with Trainer API and optimized parameters...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete. Model saved to the final checkpoint.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "tpq13OC7q_pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"\\nFinal Evaluation Results\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "-vn6DrChrC_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_DIRECTORY = \"./mental_health_status_roberta_model_assets\"\n",
        "trainer.model.save_pretrained(SAVE_DIRECTORY)\n",
        "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
        "print(f\"DistilBERT Model and Tokenizer saved to the folder: {SAVE_DIRECTORY}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZBGi5v6CrK1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER_NAME = \"mental_health_status_roberta_model_assets\"\n",
        "ZIP_NAME = \"distilbert_mental_health_roberta_model.zip\"\n",
        "!zip -r $ZIP_NAME $FOLDER_NAME\n",
        "print(f\"Folder successfully zipped as {ZIP_NAME}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SYoDFnburNOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(ZIP_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "I1zdZWdQRGqE",
        "outputId": "9d4a9c58-06f1-49d7-ef9b-80bf5b9a1ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9cc04368-5bc2-445d-a345-c366d17dc870\", \"distilbert_mental_health_roberta_model.zip\", 455425692)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}